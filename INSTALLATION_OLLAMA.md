# ğŸš€ Guide Installation Rapide - IA Gratuite Ollama

## â±ï¸ Installation en 5 minutes

### ğŸ“¥ Ã‰tape 1: TÃ©lÃ©charger Ollama (2 min)

**Windows**:
1. Va sur: https://ollama.com/download
2. Clique sur "Download for Windows"
3. Lance l'installateur `OllamaSetup.exe`
4. Suis les instructions (Next â†’ Next â†’ Install)

### ğŸ¤– Ã‰tape 2: TÃ©lÃ©charger un modÃ¨le IA (2 min)

Ouvre **PowerShell** ou **CMD** et tape:

```powershell
ollama pull llama3
```

Ou pour un modÃ¨le plus lÃ©ger (si tu as peu de RAM):

```powershell
ollama pull phi
```

**Temps d'attente**: 2-5 minutes (tÃ©lÃ©charge ~4GB)

### â–¶ï¸ Ã‰tape 3: Lancer Ollama (10 sec)

Dans PowerShell/CMD:

```powershell
ollama serve
```

Tu dois voir:
```
Ollama is running
```

**IMPORTANT**: Laisse cette fenÃªtre ouverte! Ollama tourne en arriÃ¨re-plan.

### âœ… Ã‰tape 4: VÃ©rifier l'installation (30 sec)

Dans un **NOUVEAU** PowerShell/CMD:

```powershell
cd "c:\Users\hbargaoui\OneDrive - Sopra Steria\Desktop\projet PFA\unittest-ai-agent"
python test_ollama.py
```

Tu dois voir:
```
âœ… Ollama est installÃ© et lancÃ©
âœ… Client Ollama crÃ©Ã©
âœ… Explication gÃ©nÃ©rÃ©e
âœ… Bugs dÃ©tectÃ©s
```

---

## ğŸ® Lancer ton projet avec IA

### MÃ©thode simple

1. **Terminal 1** (Ollama):
```powershell
ollama serve
```

2. **Terminal 2** (Ton projet):
```powershell
cd "c:\Users\hbargaoui\OneDrive - Sopra Steria\Desktop\projet PFA\unittest-ai-agent"
python web_app_demo.py
```

Tu verras:
```
ğŸ†“ NOUVELLES FONCTIONNALITÃ‰S IA GRATUITES (Ollama) :
   ğŸ¤– ModÃ¨le: llama3
   ğŸ’¡ Explain Code - Explique le code
   ğŸ› Detect Bugs - DÃ©tecte les bugs
   âœ¨ Improve Tests - AmÃ©liore les tests
   ğŸ¯ Edge Cases - Identifie cas limites
```

3. **Navigateur**:
```
http://127.0.0.1:5000
```

---

## ğŸ§ª Tester les nouvelles fonctionnalitÃ©s

### Via l'API

**1. VÃ©rifier le statut**:
```bash
curl http://127.0.0.1:5000/ai/status
```

RÃ©ponse attendue:
```json
{
  "available": true,
  "model": "llama3",
  "message": "âœ… Ollama prÃªt"
}
```

**2. Expliquer du code**:
```bash
curl -X POST http://127.0.0.1:5000/ai/explain ^
  -H "Content-Type: application/json" ^
  -d "{\"code\": \"public int add(int a, int b) { return a + b; }\", \"language\": \"java\"}"
```

**3. DÃ©tecter des bugs**:
```bash
curl -X POST http://127.0.0.1:5000/ai/detect-bugs ^
  -H "Content-Type: application/json" ^
  -d "{\"code\": \"public void process(String s) { s.toLowerCase(); }\", \"language\": \"java\"}"
```

---

## ğŸ› ProblÃ¨mes frÃ©quents

### âŒ "Ollama n'est pas lancÃ©"

**Solution**:
```powershell
ollama serve
```

### âŒ "model not found"

**Solution**:
```powershell
ollama pull llama3
```

### âŒ "port 11434 already in use"

**Solution**: Ollama est dÃ©jÃ  lancÃ©! C'est bon!

### âŒ RÃ©ponses trÃ¨s lentes

**Solution**: Utilise un modÃ¨le plus lÃ©ger
```powershell
ollama pull phi
```

Puis modifie `ollama_client.py` ligne 13:
```python
def __init__(self, base_url: str = "http://localhost:11434", model: str = "phi"):
```

---

## ğŸ“Š Pour ta prÃ©sentation PFA

### Points Ã  mentionner

âœ… **IA 100% gratuite** (Ollama local)  
âœ… **Pas de quota** (utilisation illimitÃ©e)  
âœ… **DonnÃ©es privÃ©es** (tout reste sur ton PC)  
âœ… **4 nouvelles fonctionnalitÃ©s IA**:
   - Explain Code
   - Detect Bugs
   - Improve Tests  
   - Edge Cases

### DÃ©mo en direct

1. Montre le serveur qui dÃ©marre avec Ollama activÃ©
2. Upload un fichier Java
3. Clique sur **"Explain Code"** â†’ Montre l'explication
4. Clique sur **"Detect Bugs"** â†’ Montre les bugs trouvÃ©s
5. Explique que **c'est gratuit et local**

---

## ğŸ¯ Comparaison avec API payantes

| CritÃ¨re | Ollama | Gemini/OpenAI |
|---------|--------|---------------|
| **CoÃ»t** | 0â‚¬ | ~0.02â‚¬/requÃªte |
| **Quota** | IllimitÃ© | LimitÃ© |
| **Vitesse** | 2-5s | 1-2s |
| **QualitÃ©** | TrÃ¨s bonne | Excellente |
| **ConfidentialitÃ©** | 100% | DonnÃ©es envoyÃ©es |
| **Installation** | 5 min | API key |

---

## ğŸ“ Liens utiles

- **Site Ollama**: https://ollama.com
- **Documentation**: https://github.com/ollama/ollama
- **ModÃ¨les**: https://ollama.com/library
- **Support**: https://discord.gg/ollama

---

## âœ¨ RÃ©sumÃ©

```bash
# 1. Installe Ollama
https://ollama.com/download

# 2. TÃ©lÃ©charge un modÃ¨le
ollama pull llama3

# 3. Lance Ollama
ollama serve

# 4. Lance ton projet
python web_app_demo.py

# 5. Teste
python test_ollama.py
```

**Temps total**: 5-10 minutes  
**RÃ©sultat**: Projet avec IA gratuite et illimitÃ©e! ğŸš€
